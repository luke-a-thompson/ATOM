[wandb]
use_wandb = false

[benchmark]
model_type = "atom"
benchmark_name = "atom_tg80_multitask_muon_fold2"
compile = true
compile_trace = false
runs = 2
log_weights = false

[dataloader]
multitask = true
delta_T = 10_000
dataset = "tg80"
num_timesteps = 8

# Single-task dataloader parameters
# molecule_type = "aspirin"  # Iterates through single-task learning on these molecules

# Multitask dataloader parameters
train_molecules = [
    "1.3-cyclohexadiene",
    "cyclohexanol",
    "benzaldehyde",
    "cyclobutane",
    "malondialdehyde2",
    "ethylamine",
    "chlorobenzene",
    "toluene",
    "methanol",
    "benzene2",
    "indole",
    "butylamine",
    "malonicacid",
    "cyclohexanone",
    "propane",
    "benzylamine",
    "formamide",
    "propylene",
    "heptanol",
    "ethylene",
    "trimethylamine",
    "biphenyl",
    "p-cresol",
    "uracil",
    "acetamide",
    "cyclopropane",
    "furfural",
    "anthracene",
    "formaldehyde",
    "salicylicacid2",
    "malondialdehyde1",
    "aspirin",
    "tropane3",
    "benzene",
    "uracil1",
    "1.3-butadiene",
]

validation_molecules = [
    "acetaldehyde",
    "1.4-dioxane",
    "cyclohexane",
    "p-xylene",
    "aniline",
    "succinicacid",
    "quinoline",
    "oxalicacid",
    "benzoicacid",
    "aceticacid",
    "tropane2",
    "cyclopentanone",
]

test_molecules = [
    "isopropanol",
    "ethanethiol",
    "2-butanone",
    "coumarin",
    "furan",
    "imidazole",
    "1.2-dichloroethane",
    "citricacid",
    "benzothiophene",
    "pentanol",
    "hexanol",
    "nitrobenzene",
]




# Other dataloader parameters
explicit_hydrogen = false
explicit_hydrogen_gradients = false
radius_graph_threshold = 1.6
rrwp_length = 8  # 0 for no RRWPs
normalize_z = false
persistent_workers = true
num_workers = 6
pin_memory = true
prefetch_factor = 2
force_regenerate = true

[training]
device = "cuda"
seed = 42
batch_size = 192
epochs = 250
use_amp = false
amp_dtype = "bfloat16"
max_grad_norm = 1.0
learned_label_noise = false
label_noise_std = 0.1

[optimizer]
type = "muon"
learning_rate = 1e-3
weight_decay = 1e-5
adam_betas = [0.9, 0.999]
adam_eps = 1e-10

[scheduler]
type = "none"

[atom_config]
# Architecture parameters
num_layers = 5
num_heads = 8
lifting_dim = 128
# Output parameters
output_heads = 8
delta_update = false
# Attention parameters
heterogenous_attention_type = "ghca"
use_rope = true
rope_base = 1000
learnable_attention_denom = false
# Feature parameters
use_spherical_harmonics = false
equivariant_lifting_type = "equivariant"
# Layer parameters
norm = "rms"
activation = "swiglu"
value_residual_type = "learnable"

[egno_config]
num_layers = 5
lifting_dim = 64
activation = "silu"
normalise_scalars = true
use_time_conv = true
num_fourier_modes = 2
time_embed_dim = 32