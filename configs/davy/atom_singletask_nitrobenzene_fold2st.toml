[wandb]
use_wandb = false

[benchmark]
model_type = "GTNO"
benchmark_name = "atom_tg80_singletask_muon_nitrobenzene"
compile = true
compile_trace = false
runs = 3
log_weights = false

[dataloader]
multitask = false
delta_T = 10_000
md17_version = "tg80"
num_timesteps = 8

# Single-task dataloader parameters
molecule_type = "nitrobenzene"  # Iterates through single-task learning on these molecules

# Multitask dataloader parameters
train_molecules = [
    "isopropanol",
    "benzoicacid",
    "cyclohexanol",
    "ethylamine",
    "heptanol",
    "butanol",
    "butylamine",
    "benzaldehyde",
    "formamide",
    "anthracene",
    "butane",
    "p-cresol",
    "ethanethiol",
    "oxalicacid",
    "indole",
    "aniline",
    "1.2-dichloroethane",
    "malonicacid",
    "salicylicacid2",
    "malondialdehyde1",
    "ethylene",
    "furfural",
    "acetamide",
    "quinoline",
    "furan",
    "methanol",
    "tropane2",
    "benzene",
    "tropane3",
    "pentanol",
    "cyclohexanone",
    "formaldehyde",
    "benzothiophene",
    "succinicacid",
    "propane",
    "chlorobenzene",
    "acetaldehyde",
]

validation_molecules = [
    "benzylamine",
    "nitrobenzene",
    "uracil1",
    "1.3-butadiene",
    "trimethylamine",
    "acetonitrile",
    "naphthalene",
    "coumarin",
]

test_molecules = [
    "2-butanone",
    "cyclopropane",
    "propylene",
    "uracil",
    "benzene2",
    "cyclohexane",
    "biphenyl",
    "1.3-cyclohexadiene",
]


# Other dataloader parameters
explicit_hydrogen = false
explicit_hydrogen_gradients = false
radius_graph_threshold = 1.6
rrwp_length = 0  # 0 for no RRWPs
normalize_z = false
persistent_workers = true
num_workers = 6
pin_memory = true
prefetch_factor = 2
force_regenerate = true

[training]
device = "cuda"
seed = 42
batch_size = 192
epochs = 1000
use_amp = false
amp_dtype = "bfloat16"
max_grad_norm = 1.0
learned_label_noise = false
label_noise_std = 0.1

[optimizer]
type = "muon"
learning_rate = 1e-3
weight_decay = 1e-5
adam_betas = [0.9, 0.999]
adam_eps = 1e-10

[scheduler]
type = "none"

[gtno_config]
# Architecture parameters
num_layers = 5
num_heads = 1
lifting_dim = 128
# Output parameters
output_heads = 8
delta_update = false
# Attention parameters
heterogenous_attention_type = "ghca"
use_rope = true
rope_base = 1000
learnable_attention_denom = false
# Feature parameters
use_spherical_harmonics = false
equivariant_lifting_type = "equivariant"
# Layer parameters
norm = "rms"
activation = "swiglu"
value_residual_type = "learnable"

[egno_config]
num_layers = 5
lifting_dim = 64
activation = "silu"
normalise_scalars = true
use_time_conv = true
num_fourier_modes = 2
time_embed_dim = 32